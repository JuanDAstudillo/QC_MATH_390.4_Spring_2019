---
By: "Juan D Astudillo" 
In Collaboration with: "Adriana Sham, Burhan Hanif, Sakib Salim, Vincent Miceli" 
title: "Term Project 390.4- 2019"
output: html_document
---
R Markdown
```{r}
pacman::p_load(dplyr, tidyr, ggplot2, magrittr, stringr, mlr, sjmisc)
housing_data = read.csv("housing_data_2016_2017.csv")
##Delete variables that we dont need
housing_data %<>%
  select(-c(HITId, HITTypeId, Title, Description, Keywords, Reward, CreationTime, MaxAssignments,   RequesterAnnotation,    AssignmentDurationInSeconds,    AutoApprovalDelayInSeconds, Expiration, NumberOfSimilarHITs, LifetimeInSeconds, AssignmentId,   WorkerId,   AssignmentStatus,   AcceptTime, SubmitTime, AutoApprovalTime,   ApprovalTime,   RejectionTime,  RequesterFeedback,  WorkTimeInSeconds, LifetimeApprovalRate,    Last30DaysApprovalRate, Last7DaysApprovalRate, URL, url, date_of_sale))
##Clean Data
housing_data %<>%
  mutate( zip_code = str_extract(full_address_or_zip_code, "[0-9]{5}")) 

housing_data %<>%
  mutate(dogs_allowed = ifelse(substr(housing_data$dogs_allowed, 1, 3) == "yes", 1, 0)) %>%
  mutate(cats_allowed = ifelse(substr(housing_data$cats_allowed, 1, 3) == "yes", 1, 0)) %>%
  mutate( pets_allowed = ifelse( cats_allowed + dogs_allowed > 0, 1, 0)) %>%
  mutate(coop_condo = factor(tolower(coop_condo)))

housing_data %<>%
  select(-c(dogs_allowed,cats_allowed, fuel_type))

d = housing_data

d %<>%
  mutate(maintenance_cost = sjmisc::rec(maintenance_cost, rec = "NA = 0 ; else = copy")) %<>%
  mutate(common_charges = sjmisc::rec(common_charges, rec = "NA = 0 ; else = copy"))##recode from NA to 0.


# combine maintaince cost and common charges
d %<>% 
  mutate( monthly_cost = common_charges + maintenance_cost)

d %<>%
  mutate(monthly_cost = sjmisc::rec(monthly_cost, rec = "0 = NA ; else = copy"))

## Garage exists conver it to binary

d %<>%
  mutate(garage_exists = sjmisc::rec(garage_exists, rec = "NA = 0 ; else = copy")) ##recode from NA to 0. 

d %<>%
  mutate(garage_exists = sjmisc::rec(garage_exists, rec = " eys = 1; UG = 1 ; Underground = 1; yes = 1 ; Yes = 1 ; else = copy")) ##recode from NA to 0.

d %<>%
  select(-c(maintenance_cost , common_charges, model_type))


summary (d)
```



```{r}
##Change variable type
d %<>%
  mutate( dining_room_type = as.factor(dining_room_type)) %>%
  mutate(garage_exists = as.character(garage_exists)) %>%
  mutate(garage_exists = as.numeric(garage_exists)) %>%
  mutate( parking_charges = as.character(parking_charges)) %>%
  mutate( parking_charges = as.numeric(parking_charges)) %>%
  mutate(sale_price = as.character(sale_price)) %>%
  mutate(sale_price = as.numeric(sale_price)) %>%
  mutate(total_taxes = as.character(total_taxes)) %>%
  mutate(total_taxes = as.numeric(total_taxes)) %>%
  mutate(price_persqft = listing_price_to_nearest_1000 / sq_footage)

#Added latitude and longitude features using ggmap
#pacman::p_load(ggmap)
#register_google(key = '************')
d %<>%
   mutate(lat = geocode(full_address_or_zip_code)$lat, lon = geocode(full_address_or_zip_code)$lon )
d %<>%
  select(-c(zip_code, full_address_or_zip_code, listing_price_to_nearest_1000))

#We are trying to predict sale_price. So let’s section our dataset:

  ####CREATE A COLUMN ID
  #miss forests will shufle rows. Created id to identify the appropiately in the future

d %<>%
  mutate(id = 1 : 2230)

real_y = data.frame(d$id, d$sale_price)

j = d %>%
  select(total_taxes)

d %<>%
  select(-c(total_taxes, sale_price))

d = cbind(j, d)

d[,1][d[, 1] < 1000] = NA ## number 1 is total taxes

real_d = subset(d, (!is.na(d[,2])))  ## sale price
fake_d = subset(d, (is.na(d[,2])))
```

```{r}
#Split the data that has y into train and test sets
train_indices = sample(1 : nrow(real_d), nrow(real_d)*4/5)
training_data = real_d[train_indices, ]
testing_data = real_d[-train_indices, ]



X = rbind(training_data, testing_data, fake_d)


#table(X$total_taxes)

#str(X)
#Let’s first create a matrix with p columns that represents missingness
M = tbl_df(apply(is.na(X), 2, as.numeric))
colnames(M) = paste("is_missing_", colnames(X), sep = "")
# head(M)
# summary(M)

#Some of these missing indicators are collinear because they share all the rows they are missing on. Let’s filter those out:
M = tbl_df(t(unique(t(M))))
#Some featuers did not have missingness so let’s remove them:
M %<>% select_if(function(x){sum(x) > 0})
# head(M)
# dim(M)
# colSums(M)

#Now let’s impute using the package. we cannot fit RF models to the entire dataset (it’s 2230! observations) so we will sample 172.
pacman::p_load(missForest)
Ximp = missForest(data.frame(X), sampsize = rep(172, ncol(X)))$ximp

Ximp %<>%
  arrange(id)


Xnew = data.frame(cbind(Ximp, M, real_y))

Xnew %<>%
  mutate(price = d.sale_price) %>%
  select(-c(id, d.id, d.sale_price))
  

linear_mod_impute_and_missing_dummies = lm(price ~ ., data = Xnew)
summary(linear_mod_impute_and_missing_dummies)
```

```{r}
##REMOVING MISSING Y SECTION

Data = Xnew
### sale price is our imputed Y

Data %<>%
  filter(!is.na(price))


Y = Data$price

Xtrain = Data[1:422, ]
Xtest = Data[423:528, ]

Ytrain = Y[1:422]
Ytest = Y[423:528]

dtrain = cbind(Xtrain, Ytrain) ## combine x train with y train, x test with y test
dtest = cbind(Xtest, Ytest)
#Linear Regression
Xtrain$price = NULL
Xtest$price = NULL
linear = lm(Ytrain ~ ., data = Xtrain)## simple linear model
summary(linear)

yhat = predict(linear, Xtest)

e = yhat - Ytest

#RMSE
sqrt(sum(e^2) / 108)
## [1] 87429.6
```

#REGRESSION TREE
```{r}
pacman::p_load(YARF)
reg_tree = YARFCART(Xtrain, Ytrain)
y_hat_test_tree = predict(reg_tree, Xtest)
e = Ytest - y_hat_test_tree
#RMSE
sqrt(sum(e^2)/108)
```
## RANDOM FORESTS
```{R}

#Make test, train and selection sets
n = nrow(Data)
K = 5
test_indices = sample(1 : n, size = n * 1 / K)
master_train_indices = setdiff(1 : n, test_indices)
select_indices = sample(master_train_indices, size = n * 1 / K)
train_indices = setdiff(master_train_indices, select_indices)
rm(master_train_indices)

houses_train = Data[train_indices, ]
houses_select = Data[select_indices, ]
houses_test = Data[test_indices, ]
#Hyperparameter Tuning for Random Forest Running this chunk gives the optimal hyperparameters used in the model.
# train_task = makeRegrTask(data = houses_train, target = "price")
# test_task = makeRegrTask(data = houses_test, target = "price")
# 
# algorithm = makeLearner("regr.randomForest", predict.type = "response")
# 
# all_mtry = seq(1, 10, by = 1)
# all_nodesize = seq(1, 10, by = 1)
# all_sampsize = seq(100, 110, by = 1)
# all_hyperparams = makeParamSet(
#   makeDiscreteParam(id = "nodesize", default = 5, values = all_nodesize),
#   makeDiscreteParam(id = "mtry", default = 5, values = all_mtry)
# )
# inner = makeResampleDesc("CV", iters = 3)
# lrn = makeTuneWrapper("regr.randomForest", 
#                       resampling = inner, 
#                       par.set = all_hyperparams, 
#                       control = makeTuneControlGrid())
# 
# 
# outer = makeResampleDesc("CV", iters = 5)
# r = resample(lrn, train_task, 
#             resampling = outer, 
#             extract = getTuneResult)
# 
# r #overall estimate of oos error of the whole procedure if it were used on all of $\mathbb{D}$
# print(getNestedTuneResultsOptPathDf(r)) #results of each inner validation over all outer iterations
# r$extract #"winning" model for each outer iteration

rf_mod = YARF(Xtrain, Ytrain, mtry = 10, nodesize = 4)
y_hat_test_tree = predict(rf_mod, Xtest)
e = Ytest - y_hat_test_tree
#RMSE
sqrt(sum(e^2)/108)
```